---
title: "Basic Statistics in R"
subtitle: "R Ladies Tbilisi Workshop"
author: "Nutsa Abazadze"
date: "2020 May"
output:
    html_document:
        code_folding: "show"
        number_sections: TRUE
        toc: true
        toc_depth: 4
        toc_float: true
        theme: flatly
        highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      collapse = TRUE,
                      comment = "#>",
                      message = FALSE,
                      options(scipen = 999),
                      options(digits=2)
)
```


> Main packages used: `pastecs`,  `psych`, `base R`, `broom`, `ggplot2`, `dplyr` <br>

> Three main topics covered: Descriptive Statistics, Inferential Statistics, Modeling

> Note, this file was made in R markdown, you can check out the .rmd file to see how this was built.

# Basic Statistical analysis and modelling in R

Packages we will need.

```{r eval=FALSE}
packages <- c("pastecs", "psych", "ggfortify", "effects", "GGally", "broom",
              "survey", "dplyr", "tidyr", "haven", 
              "gapminder", "ggplot2", "ggeffects")
install.packages(packages)
```


```{r echo=FALSE}
library(survey) # package for analyzing survey data
library(plm) # package for panel regressions
```


```{r message=FALSE}
# data import and cleaning
library(dplyr)
library(tidyr)
library(haven)
library(pastecs)
library(psych)

#statistical modelling
library(broom) # extracting model informations

# # practice data
 library(gapminder)
#
# # data visualization
library(ggplot2)
library(ggfortify)
library(effects)
library(ggeffects)
library(GGally)
```



## Before we start - Project Set-up!

Using Projcets with RStudio will simplify your workflow. Essentially, all your project related files are collected in your selected folder so you don't need to specify a working directory. Your project will be able to run as long as you copy the entire folder (or have it on the cloud).

How to set one up: `File -> New Project` then choose a directory where you want to have your R scripts, data and history files. You should also disable the **"Restore most recently opened project at startup"** and **"Restore .RData ino workspace at startup"**, aNnd also set **"Save workspace to .RData on exit"** to **Never** in `Tools -> Global Options -> General` 

For more help and materials on using projects, see [RStudio's own resource page](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects) or a [well argued reasoning from Jenny Brian](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/). 
> Let's set up a new project for this workshop!

To get started:

+ **Create a new project** for the course; I recommend working directly in Google Drive or Dropbox. Create a new folder there.
+ **Create a sub-folder for data** - call it `data` and copy the **data** we will use in this workshop. Get it from Github or USB.
+ **Start a new R script** by Ctrl + Shift + N. 
+ **Don't forget to save** the script to your project folder (Ctrl + s)! 

**You can comment your code with `#` (hash-tag). Anything in the given line after `#` will not be taken into R when it runs the code.**  


<br>
<br>


> **General tips:**  
> - Check the [R coding style guide](http://style.tidyverse.org/index.html)  
> - Comment your codes heavily (with the `#`) because now seemingly straightforward code will not be so in the future; <br>
> - Indent code with #### or `----` to create a `table of contents`; <br>
> - Use sensible file names (e.g.: `01_data_cleaning.R`);  
> - R is case sensitive, so use lowercase file and variable names. Separate words with underscore `_` (e.g.: `ols_reg_1`).


## About the dataset that we are going to use - Gapminder data

A part of the data available at Gapminder.org. For each of 142
countries, the package provides values for life expectancy, GDP per capita,
and population, every five years, from 1952 to 2007.


```{r}
remove(list = ls())
```

```{r}
mydata <- gapminder::gapminder
```

Below a preview of this dataset and its structure:

```{r}
head(mydata)
```

```{r}
str(mydata)
```


## Descriptive statistics

R provides a wide range of functions for obtaining summary statistics. 

One method of obtaining descriptive statistics is to use the sapply( ) function with a specified summary statistic. Possible functions used in sapply include mean, sd, var, min, max, median, range, and quantile.
```{r}
mean(mydata$lifeExp)
```

```{r}
# get means for variables in data frame mydata
# excluding missing values
sapply(mydata, mean, na.rm=TRUE)
```

There are also numerous R functions designed to provide a range of descriptive statistics at once. For example:
```{r}
# mean,median,25th and 75th quartiles,min,max
summary(mydata)
```

or using the "pastecs" package
```{r}
# nbr.val, nbr.null, nbr.na, min max, range, sum,
# median, mean, SE.mean, CI.mean, var, std.dev, coef.var
stat.desc(mydata)
```

another alternative is using the  "psych" package
```{R}
# item name ,item number, nvalid, mean, sd,
# median, mad, min, max, skew, kurtosis, se
describe(mydata)
```

## Student’s t-test
The t-test is a simple, yet powerful statistical technique to check whether two sample means differ from each other statistically significantly. For experimental setups you can examine if your treatment caused a statistically significant effect compared to your control group for example. The validity of the test is based on the assumptions that our sample is randomly selected from the population and people are randomly distributed between the treatment and control groups as well.

We’ll keep going with our gapminder data; we want to test if GDP is significantly different between the Americas and Europe in 2007; so we can use a basic two-sample t-test. For that question, we want to use a two-sample t-test. 

The t.test() function implements the independent samples t-test, where we have two distinct group and we want to know if their group means are different from each other. There are other variants of the t-test, such as the one-sample t-test, the paired samples t-test, but here we are focusing on the independent samples t-test.
In R a t-test is implemented in the t.test() function. As a minimum, for an independent samples t-test, you just need to provide it with two vectors of data values as the first two arguments. It also accepts input in the form of a formula, which might be more convenient in some occasions.
In general R, the formula = argument is made up by a right hand side (our dependent variable usually), which is followed by ~ and the independent variables. For the t-test, we can simply specify sample1~sample2 as our formula and supply the function with the data = argument. This way, we don’t need to specify the variable names by using $. The variable formula is used widely in R modelling and statistical analysis functions, so we should get acquinted with it.


Since we’re only interested in Europe and the Americas in 2007, we need to do a bit of filtering of the data.

```{R}
gdp_07_EuAm <- filter(gapminder, 
                      continent %in% c("Americas", "Europe"), 
                      year == 2007)
summary(gdp_07_EuAm)
```
```{r}
gdp_07_EuAm <- droplevels(gdp_07_EuAm)
head(gdp_07_EuAm)
```
```{r}
t.test(gdpPercap ~ continent, data = gdp_07_EuAm)
```

In the output we can see the value of the test statistic t = -4.8438 and the p-value = 0.00001148 Wait..what does that mean?

The t-statistic and p-value are the two most important pieces of information. They give us the same information. Every t-statistic has an associated p-value.
If the p-value of the test statistic is below a certain threshold that we have set (usually 0.05), then we can reject the null hypothesis and accept the alternative hypothesis that the true difference in means is not equal to 0 (the null hypothesis is that the difference between the sample means is 0). We can also see the confidence interval and the means for the two groups. 

In this case there is a statistically significant difference between the two continents GDP's.

## Correlation

Perhaps the most common approach to look into associations between variables is the correlation. There are also different types of correlation, here we will be talking about the Pearson correlation, which is what is usually thought of when people speak about correlation. It shows the association between two continuous variables and is implemented in R in the cor() and cor.test() functions. The first simply calculates the value of the correlation coefficient, the second also performs a statistical test to tell you if the correlation is statistically different from 0.
We will use the, by now, well known gapminder dataset for both the correlation and both the regression analysis examples. The only modifications we will make to the data is to create a cross-section of it, by limiting all observations to the year 2007.

```{r}
gapminder_cs <- subset(gapminder, year == 2007)
summary(gapminder_cs)
```

The `cor()` function is useful, because it provides the possibility to look at many variables at once. So let’s have a look at all the correlations between the variables in the dataset. The `cor()` function is picky about missing data and therefore we have to tell it to drop the cases with missing values on a variable for the calculation of a specific correlation. The `"pairwise.complete"` option tells it to use for each correlation the set of observations that complete.

In the function call, I specify that I want the correlation between the last three variables.

```{r}
cor(gapminder_cs[, 4:6], use = "pairwise.complete", method = "pearson")
```



We can test with the `cor.test` function if the estimated correlations are statistically significant. Let's check GDP per capita and population. They have a negative correlation, but we do not really know if this is significant or not. (yet)

```{r, collapse=FALSE}
cor1 <- cor.test(gapminder_cs$pop, gapminder_cs$gdpPercap, method = "pearson")
cor1
```

The p-value of the test is above our treshold of p < 0.05, thus the negative correlation between the two variable is not statistically significant. 


Let's check life expectancy and GDP per capita.

```{r}
cor.test(gapminder_cs$lifeExp, gapminder_cs$gdpPercap)
```

We can see that the correlation between the variables is clearly significant as the p-value is well below our trehsold of p < 0.05.

We can visualise correlation with scatter plots. 

> Plot the relationship between life expectancy and GDP per capita with the `ggplot` package. No need to tinker with the plot this time. For extra, you can add a trend line with the `geom_smooth()`

```{r }
ggplot(gapminder_cs, aes(gdpPercap, lifeExp)) +
    geom_point() +
    geom_smooth(method = "lm")
```

Or we can plot a correlation heatmap with the `GGally::ggcorr` function of the `GGally` ggplot extension package.

```{r}
ggcorr(gapminder_cs[, 4:6], label = TRUE)
```

## Regression

Perhaps the simplest and most common analysis one would do is linear OLS regression. It allows to model a continuous variable as a linear combination (a sum) of one or several other continuous or binary variables so that in the end we would have a rough idea about how much our response variable would change if our explanatory variable would change by a certain amount. It is a simple, but rather flexible and powerful technique and the basic linear model can be extended to cover most of the analyses one could think of. The basic OLS is good also because it is relatively understandable. Its basic principle is minimising the sum of squared differences between the actual and the predicted values.

OLS is suitable if one has a continuous response variable, which is more or less normally distributed, continuous or binary explanatory variables and a reasonable amount of cases that are independent of each other. Rules of thumb with regard to the latter differ, but it would probably not be a good idea to run a regression with less than 20 cases, especially with many explanatory variables, and one should be OK if there are more than a 100 cases and not a very large amount of predictors. The more we want out of the data, i.e. the more coefficients and relationships we are looking at, the more information (cases) we would need in order to have stable and valid estimates about the associations we are interested in.

Out of the example data that we have had, let’s try to model the life expectancy in certain countries from the `gapminder` dataset as a function of the GDP per capita, population and spatial features (continent). 

In R a linear model can be fitted with the `lm()` function, which has the same familiar arguments as the previous functions we have looked at in this section. We need to specify a formula with the response variable on the right hand side and the explanatory variables on the left hand side. And we need to tell the function the name of the data object.

```{r}
reg1 <- lm(lifeExp ~ gdpPercap + pop + continent, data = gapminder_cs)
summary(reg1)
```

The first thing we should always look at is model fit. This is shown us by the two values of R-squared at the bottom of the output. Out of these two, we should always look at adjusted R-squared, because this also takes into account the number of variables we have in the model and the number of cases that we have at our disposal. Any variable, even if there is no association at all, that is included in a model increases model fit a bit just by chance and we should account for that somehow.

Here we can see that the model fits rather well, the included independent variables help us account for 70% of the variance in a country's average population life expectancy. With such a well fitting model, we can safely move on to interpreting the coefficients. They tell us that a 1 dollar per capita increase in the GDP per capita is associated with a 0.00035 increase in life expectancy (this should make us think a bit about the nature of the relationship and its possible limits). 

We can also see that the effect size of the contient variable dwarfs the gdpPercap. This should not be a big surprise, as we cannot really claim to have a fully specified model so the continent variable is likely capturing a lot of other effects.

As in other cases, it might sometimes be better also here to present your results visually. For regressions (and linear models in general) this is made easy with the `effects` package, which can be used to isolate and plot the effect of a single variable together with its confidence intervals. The function `effect()` calculates the effect and takes the name of the variable and the model object as input and the generic `plot()` function can be used to plot the effect. Let’s see how this looks like for the model we just fitted.

```{r, collapse=FALSE}
plot(effect("gdpPercap", reg1))
plot(effect("continent", reg1))
```

Since we know that GDP per capita data is skewed, we can experiment with the log transformed variable. We can do the transformation within the formula, no need to modify the data directly.

```{r}
reg2 <- lm(lifeExp ~ log(gdpPercap) + pop + continent, data = gapminder_cs)
summary(reg2)
```



For further diagnostic plots, we can use the `ggfortify` extension of ggplot. This allows us to quickly plot key diagnostics and if needed use the `ggplot` syntax to change elements of the plots. The `ggfortify::autoplot()` figures everything out for us. You can select which plots you need with the `which = 1:6` option.

```{r}
autoplot(reg1, which = 1:6, ncol = 3, label.size = 3)
```